---
title: 树模型总结
tags:[summary]
---

## 基本：集成学习、boosting、bagging、偏差、方差、决策树是常用的基分类器、 Adaboost（Adaptive Boosting (自适应增强)）等

这一部分参加我之前的：[机器学习相关知识点总结](https://yihang-li.github.io/2021/03/13/%E7%AE%97%E6%B3%95%E9%9D%A2%E8%AF%95%E7%9B%B8%E5%85%B3%E7%9F%A5%E8%AF%86%E7%82%B9%E6%80%BB%E7%BB%93/#more)

`AdaBoost`: [参考](https://www.zhihu.com/question/323254433/answer/675554277)其自适应在于：前一个基本分类器被错误分类的样本的权值会增大，而正确分类的样本的权值会减小，并再次用来训练下一个基本分类器。同时，在每一轮迭代中，加入一个新的弱分类器，直到达到某个预定的足够小的错误率或达到预先指定的最大迭代次数才确定最终的强分类器。

**缺点：**在Adaboost训练过程中，Adaboost会使得难于分类样本的权值呈指数增长，训练将会过于偏向这类困难的样本，导致Adaboost算法易受噪声干扰。此外，Adaboost依赖于弱分类器，而弱分类器的训练时间往往很长

## 随机森林

`RF(随机森林)`：一句话-多次随机采样、多次随机选择特征属性、选取最优分割点、构建多个CART基学习器，对各学习器结果，视学习任务投票表决或做平均。

[Bagging -（有放回采样，多数表决（分类）、简单平均（回归），基学习器并联，无强依赖关系）] RF是Bagging的扩展变体，在以决策树为基学习器的基础上，进一步在决策树的训练过程中引入了随机特征选择：

- 随机选择样本（有放回采样）
- 随机选择特征属性 (有放回采样)
- 构建决策树
- 随机森林投票（平均），降低方差，增强泛化能力，防止过拟合

## GBDT

`GBDT`: (Boosting - 无放回采样，串联训练，关注被已有分类器错分的那些数据，分类结果基于所有分类器加权（权重不等）求和)

- GBDT的基本原理是Boosting里的Boosting Tree，并使用Gradient Boost
- 算法关键：利用损失函数的负梯度方向在当前模型的值作为残差的近似值，进而拟合一颗CART回归树
- GBDT会累加所有树的结果，而这种累加是无法通过分类完成的，因此GBDT的树都是CART回归树，而不是分类树（尽管GBDT调整后也可以用于分类但不代表GBDT的树为分类树) 因为Gradient Boosting 需要按照损失函数的梯度近似的拟合残差，这样拟合的是连续数值，因此只有回归树。

## XGboost

`XGBoost`: XGBoost是集成学习Boosting家族的成员，是在GBDT的基础上对boosting算法进行的改进。GBDT是用模型在数据上的负梯度作为残差的近似值，从而拟合残差。XGBoost也是拟合的在数据上的残差，但是它是用泰勒展开式对模型损失残差的近似；同时XGBoost对模型的损失函数进行了改进，并加入了模型复杂度的正则项。

## LightGBM

`LightGBM`: 基本原理和XGBoost相同，只是在框架上做了一点优化（训练速度的优化）

## 有放回采样 vs 无放回采样

🚀**Important！****理清一个概念：所谓的有放回采样，无放回采样，都是针对单次采样而言的。即：单次采样中，样本里是否会有重复样本（指在之前已经采样过的样本）**

注：如果有人拿GBDT里的无放回采样来说，100个样本，第一棵🌲采样了80个，那剩下的🌲不是就只能用20个了嘛？那他就是在混淆概念，误导你！实际是，如果规定样本大小为80，那么在无放回采样中，每次都从总体中不放回的（不会采样到重复样本）采样80个，单次采样必定不会有重复样本，但是不同批次的采样却会有！

## 相关面试问题

### 1.树模型有哪些？

### 2.随机森林、GBDT、XGboost和LightGBM有哪些区别（[参考](https://zhuanlan.zhihu.com/p/62207593)）

- `RF和Bagging对比`： RF的起始性能较差，特别当只有一个基学习器时，随着学习器数目增多，随机森林通常会收敛到更低的泛化误差。随机森林的训练效率也会高于Bagging，因为在单个决策树的构建中，Bagging使用的是‘确定性’决策树，在选择特征划分结点时，要对所有的特征进行考虑，而随机森林使用的是‘随机性’特征数，只需考虑特征的子集。

- Gradient Boosting是一种Boosting的方法，其与传统的Boosting的区别是，每一次的计算是为了减少上一次的残差(residual)，而为了消除残差，可以在残差减少的梯度(Gradient)方向上建立一个新的模型。所以说，在Gradient Boosting中，每个新的模型的建立是为了使得之前模型的残差往梯度方向减少，与传统Boosting对正确、错误样本进行加权有着很大的区别。这个梯度代表上一轮学习器损失函数对预测值求导。

- `XGBoost与GBDT的区别`： 在了解了XGBoost原理后容易理解二者的不同

  - 损失函数的改变：（导数和正则项的认识）

  - 传统的GBDT以CART树作为基学习器，XGBoost还支持线性分类器，这个时候XGBoost相当于L1和L2正则化的逻辑斯蒂回归（分类）或者线性回归（回归）；

  - 传统的GBDT在优化的时候只用到一阶导数信息，XGBoost则对代价函数进行了二阶泰勒展开，得到一阶和二阶导数；

  - XGBoost在代价函数中加入了正则项，用于控制模型的复杂度。从权衡方差偏差来看，它降低了模型的方差，使学习出来的模型更加简单，防止过拟合，这也是XGBoost优于传统GBDT的一个特性；

- `LightGBM v.s. XGBoost`

  - XGBoost采用的是level-wise的分裂策略（对每一层所有结点做无差别分裂，可能有的结点增益小，但也分裂，带来额外开销），而LightGBM采用的是leaf-wise的策略（在当前所有叶子结点中选择分裂收益最大的结点进行分裂，如此递归进行，易过拟合（因为易陷入较高深度，需对最大深度做限制，从而避免过拟合））。

  - lightgbm使用了基于histogram的决策树算法，这一点不同与xgboost中的 exact 算法（tree_method 可以使用 hist参数），histogram算法在内存和计算代价上都有不小优势。

    - 内存上优势：很明显，直方图算法的内存消耗为(#data* #features * 1Bytes)(因为对特征分桶后只需保存特征离散化之后的值)，而xgboost的exact算法内存消耗为：(2 * #data * #features* 4Bytes)，因为xgboost既要保存原始feature的值，也要保存这个值的顺序索引，这些值需要32位的浮点数来保存。
    - 计算上的优势，预排序算法在选择好分裂特征计算分裂收益时需要遍历所有样本的特征值，时间为(#data),而直方图算法只需要遍历桶就行了，时间为(#bin)； 一个子节点的直方图可以通过父节点的直方图减去兄弟节点的直方图得到，从而加速计算。

  - lightgbm支持直接输入categorical 的feature

    在对离散特征分裂时，每个取值都当作一个桶，分裂时的增益算的是”是否属于某个category“的gain。类似于one-hot编码。

  - 多线程优化：待完善





### 3.XGboost怎么处理缺失值

XGBoost没有假设缺失值一定进入左子树还是右子树，则是尝试通过枚举所有缺失值在当前节点是进入左子树，还是进入右子树更优来决定一个处理缺失值默认的方向，这样处理起来更加的灵活和合理。

### 4.随机森林的采样是有放回的吗？特征的选择是有放回的吗？（[参考](https://boosting-doc.readthedocs.io/zh_CN/latest/randomforest/)）

参见上述2.，这个问题的要点在于随机森林的`随机性`（体现在**数据集样本的随机抽样选择**和**待选特征的随机抽样选择**）。
4.1 **数据集样本的随机抽样选择**
从原始的数据集中采取有放回的抽样(bagging)，构造子数据集，子数据集的数据量是和原始数据集相同的。不同子数据集的元素可以重复，同一个子数据集中的元素也可以重复。

4.2 **待选特征的随机抽样选择**
与数据集的随机选取类似，随机森林中的子树的每一个分裂过程并未用到所有的待选特征，而是从所有的待选特征中随机选取一定的特征，之后再在随机选取的特征中选取最优的特征。（**结论：对于不同的子树而言，特征是有放回的**）

### 5. XGBoost工具支持并行。Boosting不是一种串行的结构吗?怎么并行的？

注意XGBoost的并行不是tree粒度的并行，XGBoost也是一次迭代完才能进行下一次迭代的（第t次迭代的代价函数里包含了前面t-1次迭代的预测值）。XGBoost的并行是在特征粒度上的。我们知道，决策树的学习最耗时的一个步骤就是对特征的值进行排序（因为要确定最佳分割点），XGBoost在训练之前，预先对数据进行了排序，然后保存为block结构，后面的迭代 中重复地使用这个结构，大大减小计算量。这个block结构也使得并行成为了可能，在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行。



## Reference

1. [一篇文章搞定GBDT、Xgboost和LightGBM的面试](https://zhuanlan.zhihu.com/p/148050748)
2. [GBDT算法原理深入解析](https://www.zybuluo.com/yxd/note/611571)
3. [【机器学习面试题】——随机森林+AdaBoost](https://blog.csdn.net/Heitao5200/article/details/103758643)

