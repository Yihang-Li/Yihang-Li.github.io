---
title: 算法面试相关知识点总结1
tags: [面试]
---

## 模型评估

### 分类准确率

> 分类正确的样本占总样本的比例：
>$$ \text{Accuracy} = \frac{n_{\text{correct}}}{n_{\text{total}}}$$
缺点：当数据标签失衡时，占比大的类别往往成为影响准确率的最主要因素

解决：改用平均准确率，即每个类别下的样本准确率的算术平均

### 精确率与召回率（Precision & Recall）

> 精确率是指分类正确的正样本个数(TP)占分类器判定为正样本的样本个数(TP+FP)的比例: $\frac{TP}{TP+FP}$
> 召回率是指分类正确的正样本个数(TP)占真正的正样本个数(TP + FN)的比例: $\frac{TP}{TP+FN}$
> Precision 和 Recall 之间往往存在一个权衡，它们通常用来衡量一个排序模型的性能
> 为了综合评估一个排序模型的好坏，最好绘制出模型的P-R曲线
> 它上面的一个点代表在某一阈值下，模型将大于该阈值的结果判定为正样本，小于该阈值的结果判定为负样本，此时返回结果对应的Recall 和 Precision
> 整条P-R曲线是通过将阈值从高到低移动而生成的
> 只用某个点对应的精确率和召回率是不能全面衡量模型性能的，只有通过P-R曲线的整体表现才能对模型进行更为全面的评估

### F1-score

> 精确率和召回率的调和平均值：
>
> $$ \text{F1-score} = \frac{2\times\text{precision}\times\text{recall}}{\text{precision} + \text{recall}} = \frac{TP}{TP+\frac{1}{2}(FN+FP)}$$

>
> 综合地反应一个排序模型的性能

### ROC曲线, AUC值

> ROC曲线：横坐标为假阳性率（$\text{FPR}=\frac{FP}{N}$），纵坐标为真阳性率($\text{TPR}=\frac{TP}{P}$)
> ROC曲线是通过不断移动分类器的截断点（区分正负预测结果的阈值）来生成曲线上的一组关键点的
> 当截断点选择为正无穷，即模型把样本全部预测为负类，那么FP和TP都为0，相应的FPR和TPR都为0，因此曲线的第一个点的坐标就是（0，0），其余同理

> AUC值：ROC曲线下的面积大小，量化基于ROC曲线衡量出的模型性能。
> 既然是面积，那就只需要沿着ROC的横轴积分就可以了
> ROC曲线一般都处于y=x这条直线上方（如果不是，只需把模型预测概率反转成1-p就可以得到更好的分类器），因此AUC的值一般在0.5～1之间，
> AUC越大，说明分类器越可能把真正的正样本排在前面，分类性能越好
> 另一个定义：随机给定一个正样本和一个负样本，分类器输出该正样本为正的那个概率值 比 分类器输出该负样本为正的那个概率值 要大的可能性，此时有显示表达式：$\frac{\sum I(P_{\text{正样本}}， P_{\text{负样本}})}{M\times N}$, $I(P_{\text{正样本}}， P_{\text{负样本}}) = 1 \quad \text{if} \quad P_{\text{正样本}} > P_{\text{负样本}}, 0.5 \quad \text{elif} \quad P_{\text{正样本}} = P_{\text{负样本}} $ else 0

> 当正负样本的分布发生变化时，ROC曲线的形状基本保持不变，而P-R曲线的形状一般会剧烈变化，因此ROC曲线能更稳定地反映模型本身的好坏。

### RMSE

> 常用来衡量回归模型的好坏：$\text{RMSE} = \sqrt{\frac{\sum_{i=1}^{n}(y_i-\hat{y_i})^2}{n}}$
>
> Outlier 会让RMSE指标变得很差
> 解决办法：
>
> 1. 如果认定这些离群点是噪声点，降噪处理；
> 2. 如果不认为是噪声点，需要在建模的时候考虑相应机制来提高模型的预测能力，这里比较复杂；
> 3. 更换更合适的指标，比如平均绝对百分比误差MAPE，它把每个点的误差进行了归一化，降低了离群点带来的绝对误差的影响：$\text{MAPE} = \sum_{i=1}^{n}|\frac{y_i - \hat{y_i}}{y_i}|\times\frac{100}{n}$



## 传统机器学习算法
### 决策树

> 自上而下，对样本数据进行树形分类的过程。（简单直观，解释性强）
> 结点+有向边构成：每个内部结点表示一个特征，叶结点表示类别
> 将决策树应用集成学习的思想可以得到随机森林，梯度提升决策树（GBDT）等模型
> 决策树的生成：特征选择，树的构造，树的剪枝

> 常用的决策树算法
>
> ID3：最大信息增益。
> 对于样本集合D, 类别数为K，数据集D的经验熵表示为：$H(D)=-\sum_{k=1}^{K}\frac{|C_k|}{|D|}log_2 \frac{|C_k|}{|D|}$, 其中$C_k$是样本集合D中属于第K类的样本子集
> 某个特征A对于数据集D的经验条件熵为：$H(D|A)=\sum_{i=1}^{n}\frac{|D_i|}{D}H(D_i)=\sum_{i=1}^{n}\frac{|D_i|}{D}(-\sum_{k=1}^{K}\frac{|D_{ik}|}{D_i}log_2 \frac{|D_{ik}|}{D_i})$, 其中，$D_i$表示D中特征A取第i个值的样本子集，$D_{ik}$表示$D_i$中属于第k类的样本子集
> 信息增益：$g(D,A) = H(D) - H(D|A)$, 优先选择信息增益最大的特征
>
> C4.5：最大信息增益比。
> 特征A对于数据集D的信息增益比：$g_R (D,A) = \frac{g(D,A)}{H_A(D)}$, 其中，$H_A (D) = -\sum_{i=1}^{n}\frac{|D_i|}{D} log_2 \frac{|D_i|}{D}$，称为数据集D关于A的取值熵
>
> CART：最大基尼系数（Gini）
> Gini描述的是数据的纯度，与信息熵含义类似：$\text{Gini}(D)= 1 - \sum_{k=1}^{n}(\frac{|C_k|}{|D|})^2$
每一次迭代中选取Gini指数最小的特征及其对应的切分点进行分类
  与ID3，C4.5不同的是，CART是一颗二叉树，采用二元切割法，每一步将数据按特征A的取值切成两份，分别进入左右子树，特征A的Gini指    数定义为$\text{Gini}(D|A)=\sum_{i=1}^{n}\frac{|D_i|}{|D|}Gini(D_i)$

  >ID3 会倾向于取值较多的特征
C4.5是对ID3的优化，通过引入信息增益比，在一定程度上对取值较多的特征进行惩罚，避免ID3出现过拟合的特性，提升决策树的泛化能力
样本类型角度：ID3只能处理离散型变量，而C4.5和CART都可以处理连续型变量
应用角度：ID3和C4.5只能用于分类，而CART（classification and regression tree）还可以应用于回归（使用平方误差）
ID3对样本特征缺失值比较敏感，而C4.5和CART可以对缺失值进行不同方式的处理
ID3和C4.5可以在每个结点产生多叉分支，且每个特征在层级之间不会复用；CART只产生两个分支，每个特征可以被重复使用
ID3和C4.5通过剪枝来权衡树的准确性与泛化能力，而CART直接利用全部数据发现所有可能的树结构进行对比

>剪枝：
>预剪枝（pre-pruning）：在生成决策树的过程中提前停止树的生长 （局限：有欠拟合的风险）
   1.树的深度达到一定程度，停止树的生长；
   2.到达当前结点的样本数量小于某个阈值的时候，停止树的生长；
   3.计算每次分裂对测试集的准确度提升，当小于某个阈值时，停止
>后剪枝  (post-pruning) : 让算法生成一颗完全生长的决策树，然后从最底层向上计算是否剪枝。剪枝过程将子树删除，用一个叶子结点替代，该结点的类别同样由多数投票的原则进行判断
相比于预剪枝，后剪枝通常可以得到泛化能力更强的决策树，但时间开销会更大

> 决策树生成的伪代码 (递归生成：base-如果数据集中所有样本分类一致，创建携带类标签的叶子结点；不然，寻找划分数据集的最好特征，根据其划分数据集，对于每个划分好的数据集，进行递归创建决策子树)


### 支持向量机


### 逻辑回归


## 集成学习
>将多个分类器的结果统一成一个最终的决策，其中每个单独的分类器称为基分类器
>(1) 找到误差相互独立的基分类器
>(2) 训练基分类器
>(3) 合并基分类器的结果：Voting 和 stacking 两种。前者投票多数表决的方式，后者串行的方式

### Boosting
>相当于串联基分类器，层层叠加，对前一层分错的样本给予更高的权重，是一种迭代式的学习过程，就像复习错题并改正一样。对于模型的偏差有相应的改善
训练好一个弱分类器，计算弱分类器的残差，作为下一个分类器的输入（但使得各分类器之间是强相关的，缺乏独立性，不会对降低方差有用）
比如Adaboost
又比如梯度提升决策树（GBDT）：其核心是，每一棵树学的是之前所有树结论和的残差

### Bagging （Bootstrap Aggregating）
>相当于并联基分类器，有一种D&C的思想在里面，对训练样本多次采样，分别训练出多个不同的模型，然后做综合来减小模型的方差
比如随机森林
Bagging所采用的基分类器最好是本身对样本分布较为敏感的（即不稳定的分类器），这样bagging才有用武之地


>最常用的基分类器是决策树，原因：
1. 更方便地把样本权重整合到训练过程中，不需要使用过采样的方法来调整样本权重
2. 表达能力和泛化能力可以通过调节树的层数来做折中
3. 不同子集样本集合生成的决策树基分类器随机性较大

> 偏差：模型预测值和真实值差的期望    由偏差带来的误差通常在在训练误差上就能体现出来
方差：所有模型预测值的方差    由方差带来的误差通常体现在测试误差相对于训练误差的增量上

### GBDT
梯度提升决策树。Boosting的一种，体现了从错误中学习的理念，基于决策树预测的残差进行迭代的学习
>Gradient Boosting是Boosting的一大类算法，其基本思想是根据当前模型损失函数的负梯度信息来训练新加入的弱分类器，然后将训练好的弱分类器以累加的形式结合到现有模型中
在每一轮迭代中，首先计算出当前模型在所有样本上的负梯度，然后以该值为目标训练一个新的弱分类器进行拟合并计算出该弱分类器的权重，最终实现对模型的更新
GBDT中使用的决策树通常为CART

> 梯度提升 and 梯度下降
梯度下降：模型以参数化的形式表示，模型的更新等价于参数的更新
梯度提升：模型并不需要参数化表示，直接定义在函数空间中，从而大大扩展了可以使用的模型种类

>GBDT优缺点
优点：1. 预测阶段计算速度快，树与树之前可以并行化计算；2. 分布稠密的数据集上，泛化能力和表达能力都很好；3. 采用决策树作为弱分类器使得其具有较好的解释性和鲁棒性，能自动发现特征间的高阶关系，不需要对数据进行特殊预处理
缺点：1. 在高维稀疏数据上表现不如SVM或者神经网络； 2. 处理文本分类特征问题上的优势不如处理数值特征时明显；3. 训练过程需要串行训练

### XGboost
实现了对GBDT算法的改进
原始的GBDT算法基于经验损失函数的负梯度来构造新的决策树，只是在决策树构建完成后再进行剪枝
而XGBoost在决策树构建阶段就加入了正则项
XGBoost有特定的准则来选取最优分裂
>与GBDT的联系和区别：
1. GBDT是机器学习算法；XGBoost是该算法的工程实现
2. 使用CART作为基分类器时，XGBoost显示地加入了正则项来控制模型的复杂度，有利于防止过拟合
3. GBDT在模型训练时只使用了损失函数的一阶导数信息，而XGBoost对损失函数进行二阶泰勒展开，可以同时使用一阶和二阶导数
4. 传统的GBDT采用CART作为基分类器，而XGBoost支持多种类型的基分类器，比如线性的
5. 传统的GBDT在每轮迭代时使用全部数据，而XGBoost则可以支持对数据采用
6. 传统的GBDT没有设计对缺失值进行处理，而XGBoost能够自动学习出缺失值处理侧拉



