---
title: 深度学习相关知识点总结
tags:[summary]
---

问题导向

问答式总结

<!-- more -->

## CNN

前馈神经网络的一种，主要由卷积层构成，具有`局部连接（稀疏交互）`和`权值共享（参数共享）`等特性，主要用于图像识别领域

### CNN相对全连接神经网络有什么优势？

- 全连接神经网络， 输出层每个结点与输入层每个结点相连，参数数量随着输入图片尺寸的增加而大大增加
- CNN，通过图片的一些模式(pattern)来识别整张图片，而识别这些pattern不需要看整张图片
  相同的pattern可能会出现在图片的不同位置，对图片进行降采样（池化）不会改变图片的特征，带来的好处是需要的参数量大大减小

### CNN怎么求卷积？

- `卷积运算（Convolutional Operator）`
  $$
  s(t) = (x*w)(t) = \int x(a)w(t-a)da=\sum_{a=-\infty}^{\infty}x(a)w(t-a)
  $$

  第二个等号为连续情形；第三个等号为离散情形
  $x$称为输入，$w$称为核函数，也叫filter，输出有时称作特征映射(feature map)
  注：

  - 卷积是可交换的，这是因为将核相对于输入进行翻转(flip)；（上下翻转，再左右翻转，再对应项相乘相加）
  - 通常使用的互相关函数(cross-crorrelation)，和卷积运算几乎一样但是没有对核进行翻转
  - 实际中，对应项相乘相加
  - 在深度学习中核数组都是学习出来的：卷积层无论使用互相关运算或卷积运算都不影响模型预测时的输出

### 请解释一下CNN的通道？

- `通道（channel）` 例如对RGB图片来说，channel的数量是3（分别对应R、G、B），而对于单色图片来说channel的数量是1. 总体而言，channel可以分为下述三种：
  - 最初输入图片样本的in_channel，取决于图片类型
  - 卷积操作完成后输出的out_channel, 取决于卷积核的数量（此时的out_channel也会作为下一次卷积时卷积核的in_channel）
  - 卷积核中的in_channel. 即上一次卷积的out_channel, 或者当第一次卷积时，为对应图片样本的channel

> 具体来说，卷积层是通过特定数目的卷积核（filter）对输入的多通道（channel）特征图进行扫描和运算，从而得到多个拥有更高层语义信息的输出特征图（通道数目等于卷积核个数）

[参考](https://xinghanzzy.github.io/2019/04/11/CNN%20channel%20and%20conv/)

> ```
> 四个通道上的卷积操作:
> 	有两个卷积核，生成两个通道。
> 	其中需要注意的是，四个通道上每个通道对应一个2*2的卷积核
> 	这4个2*2的卷积核上的参数是不一样的，之所以说它是1个卷积核，是因为把它看成了一个4*2*2的卷积核，4代表一开始卷积的通道数，2*2是卷积核的尺寸，实际卷积的时候其实就是4个2*2的卷积核（这四个22的卷积核的参数是不同的）分别去卷积对应的4个通道，然后相加，再加上偏置b，注意b对于这四通道而言是共享的，所以b的个数是和最终的featuremap的个数相同的，然后再取激活函数
>     
> 	输出层的卷积核个数为 feature map 的个数。也就是说卷积核的个数=最终的featuremap的个数，卷积核的大小=开始进行卷积的通道数每个通道上进行卷积的二维卷积核的尺寸（此处就是4（2*2）），b（偏置）的个数=卷积核的个数=featuremap的个数。
> 
> 4个通道卷积得到2个通道的过程中，参数的数目为4×（2×2）×2+2个，其中4表示4个通道，第一个2*2表示卷积核的大小，第三个2表示生成的featuremap个数，也就是得到的2通道的feature map，也就是生成的通道数，最后的2代表偏置b的个数。
> ```

`感受野（Receptive Field)` 对于*某层输出特征图*上的某个点，在卷积神经网络的*原始输入数据*上能影响到这个点的取值的区域

> 可以通过更深的卷积神经网络使特征图中单个元素的感受野变得更加广阔，从而捕捉输入上更大尺寸的特征

`步幅（Stride）`卷积核窗口每次滑动的行数和列数（No-Stride: Stride = 1）

`填充（Padding）`是指在输入高和宽（二维情形）的两侧填充元素（通常是0元素，zero-padding）

### 请简单阐述一下CNN的稀疏交互和参数共享

- `稀疏交互` 卷积核尺度远小于输入的维度，每个输出神经元仅与前一层特定区域内的神经元存在连接权重（即产生交互）
  注：让网络变得简单，可以缓解过拟合；物理意义：先学习局部特征，再把局部特征组合起来形成更复杂和抽象的特征

- `参数共享` 在同一个模型的不同模块中使用相同的参数（即卷积核），卷积运算的固有属性
  注：大大降低了模型的存储需求；物理意义：参数共享使得卷积层具有平移等变性（equivariance）

### 请阐述一下批量归一化

### [Batch Normalization](https://nealjean.com/ml/neural-network-normalization/)

- `Covariate Shift`: 模型的训练集和测试集的分布不一致，或者模型在训练过程中输入数据的分布发生变化

  对于一个复杂的机器学习系统，在训练过程中一般会要求系统里各个子模块的输入分布是稳定的，如果不满足，则称为内部协变量偏移（Internal covariant shift），网络越深，这种现象越明显

- 而BN， 通过对每一个批量的数据进行z-score归一化，也就是利用该批量的均值和方差，来减小因内部协变量偏移带来的影响
- 采用批量归一化后，深度神经网络的训练过程更加稳定、对初始值不再那么敏感、可以采用较大的学习率来加速收敛

- BN可以看作是带参数的标准化：$y^{(k)} = \gamma^{(k)}\frac{x^{(k)}-\mu^{(k)}}{\sqrt{(\sigma^{(k)})^2+\epsilon}}+\beta^{(k)}$. 其中，$x^{(k)}, y^{(k)}$分别是原始输入数据和BN后的输出数据，$\mu^{(k)}, \sigma^{(k)}$分别是输入数据的均值和标准差（在mini-batch上），$\beta^{(k)}, \gamma^{(k)}$分别是可学习的平移参数和缩放参数，上标$k$表示数据的第$k$维（BN在数据各个维度上是独立进行的），$\epsilon$是为防止分母为0的一个小量

- BN 在网络中的位置有争议，但是一般来说是放在卷积层之后，池化层之前

### 请阐述一下池化

## Pooling

- `池化` 本质是降采样，能显著降低参数量（降维），还能保持对平移、伸缩、旋转操作的不变性。
  注：池化主要是从降低计算复杂度的角度来考虑，但是当计算资源充沛的时候，有些任务不进行池化会有更好的效果，而且对另一些任务来说，是不能进行池化操作的，i.e. 下围棋，Alpha Go（详见李宏毅）

- 常用池化：

1. Mean Pooling（对池化窗口求均值）: 能够抑制由于领域大小受限造成的估计值方差增大的现象，特点是对背景的保留效果更好
2. max pooling（取池化窗口内的最大值）：能够抑制网络参数误差造成估计均值偏移的现象，特点是更好地提取纹理信息

### 关于CNN的计算

## Computation

假设一个卷积层输入特征图的尺寸为$l_w^{(i)}\times l_h^{(i)}$，卷积核大小为$k_w \times k_h$，步长为$s_w \times s_h$，则输出特征图的尺寸$l_w^{(o)}\times l_h^{(o)}$ 如何计算？

Note：假设在卷积核的滑动过程中，我们对输入特征图的左右两侧分别进行了$p_w$列填充，上下两侧分别进行了$p_h$行填充，填充后的特征图尺寸为$(l_w^{(i)}+2p_w)\times (l_h^{(i)}+2p_h)$，则输出特征图的尺寸为：

$$
l_e^{(o)} = \left\lfloor\frac{l_e^{(i)} + 2p_e - k_e}{s_e}\right\rfloor + 1, e \in \{w, h\}
$$

如果输入特征图的通道数为$c^{(i)}$，输出特征图的通道数为$c^{(o)}$，在不考虑偏置项（bias）的情况下，卷积层的参数量和计算量是多少？

参数量：卷积层的参数量，主要取决于每个卷积核的参数量以及卷积核的个数。这里，每个卷积核含有$c^{(i)}k_wk_h$个参数，而卷积核的个数即输出特征图的通道个数$c^{(o)}$，因此参数总量为：

$$
c^{(i)}c^{(o)}k_wk_h
$$

计算量：卷积层的计算量，由卷积核在每个滑动窗口内的计算量以及整体的滑动次数决定。在每个滑动窗口内，卷积操作的计算量大约为$c^{(i)}k_wk_h$，而卷积核的滑动次数即输出特征图的数据个数，也就是$c^{(o)}l_w^{(o)}l_h^{(o)}$，因此整体的计算量为：

$$
c^{(i)}c^{(o)}l_w^{(o)}l_h^{(o)}k_wk_h
$$

## 



## RNN

### 什么是RNN？

区别于前馈神经网络，是一类用于处理`序列数据`的网络结构，输入通常是连续的、长度不固定的序列数据

注：

1. 传统的机器学习方法中，序列建模常用隐马尔可夫模型（HMM）和条件随机场（CRF）
2. 近年来，RNN应用领域：机器翻译、序列标注、图像描述、推荐系统、智能聊天机器人、自动作词作曲等
3. 就像几乎所有函数都可以被认为是前馈网络，本质上任何涉及循环（$s^{(t)}=f(s^{(t-1)};\theta)$, $s$在t时刻的定义需要参考t-1时刻同样的定义）的函数都可以视为一个循环神经网络

### 为什么选择RNN

传统的前馈神经网络一般的输入都是一个定长的向量，无法处理变长的序列信息，即使通过处理变成定长的的向量，也很难捕捉到序列中的长距离依赖关系

RNN能够较好地处理序列信息，并能捕获长距离样本之间的关联信息，此外还能用隐结点状态保存序列中有价值的历史信息，使得网络能够学习到整个序列的浓缩的、抽象的信息

### RNN如何运作

	1. 假设样本序列之间有依赖关系
 	2. 认为最近的样本比之前的样本能提供更多的信息

通过将神经元串行起来处理序列化的数据，每个神经元能用它的内部变量保存之前输入的序列信息，整个序列被浓缩成抽象的表示，并可以据此进行分类或生成新的序列

循环神经网络的设计思路和卷积神经网络类似。
为了处理序列数据，RNN设计了循环/重复的结构，这部分结构通常称为事件链
事件链间存在依赖关系，即t时刻的定义及计算需要参考t-1时刻的定义及计算

### RNN中的梯度消失和爆炸问题

- 传统的RNN梯度可以表示成连乘的形式
- 对应雅可比矩阵的最大特征值大于1时，随着离输出越来越远，每层的梯度大小会呈指数增长，导致梯度爆炸
  - 可以通过梯度裁剪来缓解（即当梯度的范数大于某个域值时，对梯度进行等比收缩）
- 反之，若小于1， 会呈指数缩小，产生梯度消失，使得RNN很难学习到输入序列中的长距离依赖关系
  - LSTM，GRU等模型通过加入门控机制，弥补了梯度消失带来的损失

### RNN中能否使用ReLu作为激活函数？

可以，但需要对矩阵初值做一定限制，否则容易引发数值问题



### LSTM

RNN面临着长期依赖问题，即随着输入序列长度的增加，网络无法学习和利用序列中较久之前的信息

`1997 LSTM` 敏感地应对短期信息+对有价值的信息进行长期记忆 （一个简单变体：`GRU` (Gated Recurrent Unit)， 门控循环单元）

LSTM 和 GRU能较好地解决梯度消失或梯度爆炸问题

#### LSTM如何实现长短期记忆？
输入们、遗忘门、输出门、 和一个内部记忆单元

- 输入门控制当前计算的新状态以多大程度更新到记忆单元
- 遗忘门控制前一步记忆单元中的信息有多大程度被遗忘掉
- 输出门控制当前输出有多大程度取决于当前的记忆单元
- 在一个训练好的网络中，当输入序列没有重要信息时，LSTM的遗忘门的值接近于1，输入门的值接近于0，过去的记忆被保存，从而实现了长期记忆
- 当有重要信息的时候，就把它存入记忆中，此时输入门的值接近于1
- 当有重要信息而且之前的信息不再重要时，输入门的值接近于1，遗忘门的值接近于0

#### LSTM各个模块分别使用什么激活函数？

- 遗忘门、输入门、输出门使用Sigmoid激活函数  0～1
- 生成候选记忆时，使用双曲正切函数Tanh作为激活函数 
- 它们都是饱和的激活函数，易于控制门开或门关，使用非饱和的就不容易控制
- 原始的LSTM使用的是sigmoid的变种，一个线性变换