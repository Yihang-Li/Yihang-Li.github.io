---
title: 算法相关知识点总结
tags: [总结]
---

## 模型评估

<!-- more -->

### 分类准确率

> 分类正确的样本占总样本的比例：(等价于0-1 损失)
>$$ \text{Accuracy} = \frac{n_{\text{correct}}}{n_{\text{total}}}$$
缺点：当数据标签失衡时，占比大的类别往往成为影响准确率的最主要因素

解决：改用平均准确率，即每个类别下的样本准确率的算术平均

### 精确率与召回率（Precision & Recall）

> 精确率是指分类正确的正样本个数(TP)占分类器判定为正样本的样本个数(TP+FP)的比例: $\frac{TP}{TP+FP}$
> 召回率是指分类正确的正样本个数(TP)占真正的正样本个数(TP + FN)的比例: $\frac{TP}{TP+FN}$
> Precision 和 Recall 之间往往存在一个权衡，它们通常用来衡量一个排序模型的性能
> 为了综合评估一个排序模型的好坏，最好绘制出模型的P-R曲线
> 它上面的一个点代表在某一阈值下，模型将大于该阈值的结果判定为正样本，小于该阈值的结果判定为负样本，此时返回结果对应的Recall 和 Precision
> 整条P-R曲线是通过将阈值从高到低移动而生成的
> 只用某个点对应的精确率和召回率是不能全面衡量模型性能的，只有通过P-R曲线的整体表现才能对模型进行更为全面的评估

### F1-score

> 精确率和召回率的调和平均值：
>
> $$ \text{F1-score} =\frac{2}{\text{precision}^{-1}+\text{recall}^{-1}} = 2\cdot\frac{\text{precision}\times\text{recall}}{\text{precision} + \text{recall}} = \frac{TP}{TP+\frac{1}{2}(FN+FP)}$$

>
> 综合地反应一个排序模型的性能

### ROC曲线, AUC值

> ROC曲线：横坐标为假阳性率（$\text{FPR}=\frac{FP}{N}$），纵坐标为真阳性率($\text{TPR}=\frac{TP}{P}$)
> ROC曲线是通过不断移动分类器的截断点（区分正负预测结果的阈值）来生成曲线上的一组关键点的
> 当截断点选择为正无穷，即模型把样本全部预测为负类，那么FP和TP都为0，相应的FPR和TPR都为0，因此曲线的第一个点的坐标就是（0，0），其余同理

> AUC值：ROC曲线下的面积大小，量化基于ROC曲线衡量出的模型性能。
> 既然是面积，那就只需要沿着ROC的横轴积分就可以了
> ROC曲线一般都处于y=x这条直线上方（如果不是，只需把模型预测概率反转成1-p就可以得到更好的分类器），因此AUC的值一般在0.5～1之间，
> AUC越大，说明分类器越可能把真正的正样本排在前面，分类性能越好
> 另一个定义：随机给定一个正样本和一个负样本，分类器输出该正样本为正的那个概率值 比 分类器输出该负样本为正的那个概率值 要大的可能性，此时有显示表达式：$\frac{\sum I(P_{\text{正样本}}， P_{\text{负样本}})}{M\times N}$, $I(P_{\text{正样本}}， P_{\text{负样本}}) = 1 \quad \text{if} \quad P_{\text{正样本}} > P_{\text{负样本}}, 0.5 \quad \text{elif} \quad P_{\text{正样本}} = P_{\text{负样本}} $ else 0

> 当正负样本的分布发生变化时，ROC曲线的形状基本保持不变，而P-R曲线的形状一般会剧烈变化，因此ROC曲线能更稳定地反映模型本身的好坏。

### RMSE

> 常用来衡量回归模型的好坏：$\text{RMSE} = \sqrt{\frac{\sum_{i=1}^{n}(y_i-\hat{y_i})^2}{n}}$
>
> Outlier 会让RMSE指标变得很差
> 解决办法：
>
> 1. 如果认定这些离群点是噪声点，降噪处理；
> 2. 如果不认为是噪声点，需要在建模的时候考虑相应机制来提高模型的预测能力，这里比较复杂；
> 3. 更换更合适的指标，比如平均绝对百分比误差MAPE，它把每个点的误差进行了归一化，降低了离群点带来的绝对误差的影响：$\text{MAPE} = \sum_{i=1}^{n}|\frac{y_i - \hat{y_i}}{y_i}|\times\frac{100}{n}$


## 回归
### Least Square
>有闭形式的解：$(X^{\top}X)^{-1}X^{\top}y$ （当括号内可逆时）

### Ridege
> $L_2$正则化
> 也有闭形式的解：$(X^{\top}X + \lambda I)^{-1}X^{\top}y$ （适当的$\lambda$使得括号内一定可逆）

### Lasso
> $L_1$正则化，稀疏性，不可求梯度
> 求解：Proximal Gradient Descent （= proximal point + gradient descent）分成可以求梯度和不可以求梯度的两部分，对于前者用线性近似，后者用函数本身, 得到新的目标函数之后，分解，对求和中的每一项分别求最小，就可以得到相应的参数对应的分量的迭代表达式，而且是闭形式的
>
### P-Value
给定原假设和备择假设，在假定原假设成立的情况下，构造相应的检验统计量，计算出对应的P-value，如果P-value很小，低于我们预先给定的显著性水平，说明小概率事件发生了，而现实中小概率事件发生往往是不太可能的，所以我们应该拒绝原假设，接受备择假设

P-Value不用于证明任何事情。、**当结果具有统计学意义时，p值将用作质疑我们最初假设（无效假设）的工具**

## 传统机器学习算法
### 决策树 （DT）

> 自上而下，对样本数据进行树形分类的过程。（简单直观，解释性强）
> 结点+有向边构成：每个内部结点表示一个特征，叶结点表示类别
> 将决策树应用集成学习的思想可以得到随机森林，梯度提升决策树（GBDT）等模型
> 决策树的生成：特征选择，树的构造，树的剪枝

> 常用的决策树算法
>
> ID3：最大信息增益。
> 对于样本集合D, 类别数为K，数据集D的经验熵表示为：$H(D)=-\sum_{k=1}^{K}\frac{|C_k|}{|D|}log_2 \frac{|C_k|}{|D|}$, 其中$C_k$是样本集合D中属于第K类的样本子集
> 某个特征A对于数据集D的经验条件熵为：$H(D|A)=\sum_{i=1}^{n}\frac{|D_i|}{D}H(D_i)=\sum_{i=1}^{n}\frac{|D_i|}{D}(-\sum_{k=1}^{K}\frac{|D_{ik}|}{D_i}log_2 \frac{|D_{ik}|}{D_i})$, 其中，$D_i$表示D中特征A取第i个值的样本子集，$D_{ik}$表示$D_i$中属于第k类的样本子集
> 信息增益：$g(D,A) = H(D) - H(D|A)$, 优先选择信息增益最大的特征
>
> C4.5：最大信息增益比。
> 特征A对于数据集D的信息增益比：$g_R (D,A) = \frac{g(D,A)}{H_A(D)}$, 其中，$H_A (D) = -\sum_{i=1}^{n}\frac{|D_i|}{D} log_2 \frac{|D_i|}{D}$，称为数据集D关于A的取值熵
>
> CART：最大基尼系数（Gini）
> Gini描述的是数据的纯度，与信息熵含义类似：$\text{Gini}(D)= 1 - \sum_{k=1}^{n}(\frac{|C_k|}{|D|})^2$
每一次迭代中选取Gini指数最小的特征及其对应的切分点进行分类
  与ID3，C4.5不同的是，CART是一颗二叉树，采用二元切割法，每一步将数据按特征A的取值切成两份，分别进入左右子树，特征A的Gini指    数定义为$\text{Gini}(D|A)=\sum_{i=1}^{n}\frac{|D_i|}{|D|}Gini(D_i)$

  >ID3 会倾向于取值较多的特征
C4.5是对ID3的优化，通过引入信息增益比，在一定程度上对取值较多的特征进行惩罚，避免ID3出现过拟合的特性，提升决策树的泛化能力
样本类型角度：ID3只能处理离散型变量，而C4.5和CART都可以处理连续型变量
应用角度：ID3和C4.5只能用于分类，而CART（classification and regression tree）还可以应用于回归（使用平方误差）
ID3对样本特征缺失值比较敏感，而C4.5和CART可以对缺失值进行不同方式的处理
ID3和C4.5可以在每个结点产生多叉分支，且每个特征在层级之间不会复用；CART只产生两个分支，每个特征可以被重复使用
ID3和C4.5通过剪枝来权衡树的准确性与泛化能力，而CART直接利用全部数据发现所有可能的树结构进行对比

>剪枝：
>预剪枝（pre-pruning）：在生成决策树的过程中提前停止树的生长 （局限：有欠拟合的风险）
   1.树的深度达到一定程度，停止树的生长；
   2.到达当前结点的样本数量小于某个阈值的时候，停止树的生长；
   3.计算每次分裂对测试集的准确度提升，当小于某个阈值时，停止
>后剪枝  (post-pruning) : 让算法生成一颗完全生长的决策树，然后从最底层向上计算是否剪枝。剪枝过程将子树删除，用一个叶子结点替代，该结点的类别同样由多数投票的原则进行判断
相比于预剪枝，后剪枝通常可以得到泛化能力更强的决策树，但时间开销会更大

> 决策树生成的伪代码 (递归生成：base-如果数据集中所有样本分类一致，创建携带类标签的叶子结点；不然，寻找划分数据集的最好特征，根据其划分数据集，对于每个划分好的数据集，进行递归创建决策子树)


### 支持向量机 (SVM)
> [参考](https://zhuanlan.zhihu.com/p/84796233)
> 一句话总结：最大化分类间隔的线性分类器（不使用核函数时）
SVM有三宝，支撑（支持向量），间隔（分类间隔），核技巧（将低维非线性数据转换为高维线性数据的同时避免高维计算）；
SVM的分类结果仅依赖于支持向量；
超平面分离定理：对于两个不相交的凸集，存在一个超平面，将两个凸集分离



### 逻辑回归 (LR)
> 一句话总结：假定数据服从逻辑斯蒂分布，利用MLE来求解参数
与线性回归的联系：如果把一个事件的几率（odds）定义为该事件发生的概率与不发生的概率的比值，$\frac{p}{1-p}$，那么逻辑回归可以看作是对于$y=1|x$这一事件的对数几率的线性回归
  Since $\text{Pr}[y=+1|x;\theta] = h(\theta^{\top}x)=\frac{1}{1+e^{-\theta^{\top}x}}, \quad \text{Pr}[y=-1|x;\theta] = 1 - \text{Pr}[y=+1|x;\theta] = \frac{1}{1+e^{\theta^{\top}x}}$. Thus,  $\text{Pr}[y|x;\theta] = \frac{1}{1+e^{-y\theta^{\top}x}}$
假设我们学习到了$\theta$，则当$\text{Pr}[y|x;\theta] = \frac{1}{1+e^{-y\theta^{\top}x}} \ge \frac{1}{2}$ 时，我们把数据划分为正类，由这个式子可以推出等价于 $\theta^{\top}x \ge 0$.
通过MLE，我们可以推导出Logistic损失函数：
$$
\frac{1}{n}\sum_{i=1}^{n}log(1+exp(-y_i\cdot\theta^{\top}x_i))
$$
没有闭形式的解，但因为是凸函数，可以用基于梯度的优化方法求解，且解一定是quan ju zui you j


**降维**
>降维：即用一个低维度的向量表示原始高维度的特征
常见的降维方法：PCA、LDA、等距映射、局部线性嵌入、拉普拉斯特征映射、局部保留投影

### 主成分分析 (PCA)
>线性、非监督
>PCA旨在找到数据的主成分，用之表征原始数据，达到降维的目的：找到一个投影方向，最大化投影方差（数据分布得更为分散）
>Tips：样本投影后的方差就是对应协方差矩阵的特征值，我们要找的最大方差对应于协方差矩阵的最大特征值，最佳投影方向就是相应的特征向量（次佳投影方向位于最佳投影方向的正交空间中，即第二大特征值对应的特征向量）
- 中心化样本数据
- 求样本协方差矩阵
- 对协方差矩阵进行特征值分解，并将其从大到小进行排序
- 取特征值前$d$ 大对应的特征向量$w_1, w_2, \dots, w_d$，通过以下映射将$n$ 维样本映射到$d$ 维：$x_i^{\prime} = [w_1^{\top}x_i, w_2^{\top}x_i, \dots, w_d^{\top}x_d]^{\top}$
定义降维后的信息占比：$\eta = \sqrt{\frac{\sum_{i=1}^d \lambda_i^2}{\sum_{i=1}^n \lambda_i^2}}$
注： 可以通过核映射对PCA进行扩展得到核主成分分析（KPCA），也可以通过流形映射的降维方法，比如等距映射，局部线性嵌入，拉普拉斯特征映射等对一些PCA效果不好的复杂数据集进行非线性降维操作
>另一个角度：最小平方误差（待完善）



### 线性判别分析（LDA）
>线性、有监督
>LDA为分类服务，旨在找到一个投影方向，使得投影后的样本尽可能按照原始类别分开：最大化类间距离和最小化类内距离
> 类间距离：$\|w^{\top}(\mu_1-\mu2)\|_2^2$，将整个数据集的类内方差定义为各个类分别的方差之和：$D_1+D_2$
> 将目标函数定义为类间距离和类内距离的比值，从而需要最大化的目标： $$ max_w J(w) = \frac{\|w^{\top}(\mu_1-\mu2)\|_2^2}{D_1+D_2}$$
> （后续推导待整理）
> 经过推导，我们最大化的目标对应了一个矩阵的特征值，于是LDA降维变成了一个求矩阵特征向量的问题
>  Notes：LDA对数据的分布做了很强的假设-每个类数据都是高斯分布，各个类的协方差相等；模型简单，表达能力有一定局限性，可以通过引入核函数扩展LDA方法以处理分布较为复杂的数据



### K近邻 （KNN）
>对于给定的训练数据集和新的样本，在训练数据集中找出与这个新的样本距离最近的k个样本点，然后多数投票原则决定该新样本点所属的分类
>三要素：1. K值的选择（越小模型越复杂，交叉检验）；2.距离度量的选择（欧氏距离，马氏距离）； 3. 分类决策规则（多数表决）
>KNN回归：在找到最近的k个实例之后，可以计算这k个实例的平均值作为预测值。或者还可以给这k个实例添加一个权重再求平均值，这个权重与度量距离成反比（越近权重越大）
>KNN算法的优点：1.思想简单，理论成熟，既可以用来做分类也可以用来做回归；2.可用于非线性分类；3.训练时间复杂度为O(n)；3.准确度高，对数据没有假设，对outlier不敏感；
缺点：1.计算量大；2.样本不平衡问题（即有些类别的样本数量很多，而其它样本的数量很少）；3.需要大量的内存；
KD树：（待整理）


### 朴素贝叶斯 （Naive Bayes）
>通过预测指定样本属于特定类别的概率$P(y_i|x)$来预测该样本的所属类别，即：$y = \text{argmax}_{y_i}P(y_i|x) (=\frac{P(x|y_i)P(y_)}{P(x)} $


### K均值聚类 （K-Means）
>通过迭代方式寻找K个簇（cluster）的一种划分方案，使得聚类结果对应的损失函数最小
损失函数可以定义为各个样本距离所属簇中心点的误差平方和：$J(c,\mu)=\sum_{i=1}^{n}\|x_i-\mu_{c_i}\|^2$, 其中$x_i$ 代表第$i$ 个样本，$c_i$是$x_i$所属的簇，$\mu_{c_i}$代表簇对应的中心点，$n$ 为样本总数。

**算法步骤**
1. 数据预处理，如归一化、离群点处理等
2. 随机选取K个簇中心，记为$\mu_1^{(0)}, \mu_2^{(0)}, \dots, \mu_K^{(0)}$
3. 定义损失函数：$J(c,\mu)=\sum_{i=1}^{n}\|x_i-\mu_{c_i}\|^2$
4. 令$t=0, 1, 2, \dots$ 为迭代步数，重复下面过程直到J收敛：
   4.1 对于每一个样本$x_i$，将其分配到距离最近的簇：$c_i^{(t)} \leftarrow \text{argmin}_k\|x_i - \mu_k^{(t)}\|^2$
   4.2 对于每一个类簇，重新计算该类簇的中心：$\mu_k^{(t+1)} \leftarrow \text{argmin}_{\mu}\sum_{i:c_i^{(t)}=k}\|x_i - \mu\|^2$
**优缺点**
缺点：受初始值和离群点的影响每次的结果都不稳定，得到的解通常不是全局最优而是局部最优，无法很好解决数据簇分布差别比较大的情况，不太适用于离散分类等
优点：计算复杂度接近于线性 O(nKt)
**调优**
1. 数据归一化和离群点处理
2. 合理选取K值：如手肘法（经验方法）， Gap Statistic方法（Gap(K) = E(logD_k) - logD_k, 只需找到最大Gap statistic所对应的K即可)
3. 采用核函数  核K均值算法（核聚类方法的一种）

**改进的模型**
K-means++：（从改进初始值的角度）假设已经选取了n（0<n<K）个初始聚类中心，则在选取下一个聚类中心时，距离当前n个聚类中心越远的点会有更高的概率被选为下一个聚类中心
ISODATA算法：（当K值的大小不确定时）迭代自组织数据分析法 详见百面机器学习
**手写Kmeans代码**
待补充



## 集成学习
>将多个分类器的结果统一成一个最终的决策，其中每个单独的分类器称为基分类器
>(1) 找到误差相互独立的基分类器
>(2) 训练基分类器
>(3) 合并基分类器的结果：Voting 和 stacking 两种。前者投票多数表决的方式，后者串行的方式

### Boosting
>相当于串联基分类器，层层叠加，对前一层分错的样本给予更高的权重，是一种迭代式的学习过程，就像复习错题并改正一样。对于模型的偏差有相应的改善
训练好一个弱分类器，计算弱分类器的残差，作为下一个分类器的输入（但使得各分类器之间是强相关的，缺乏独立性，不会对降低方差有用）
比如Adaboost
又比如梯度提升决策树（GBDT）：其核心是，每一棵树学的是之前所有树结论和的残差

### Bagging （Bootstrap Aggregating）
>相当于并联基分类器，有一种D&C的思想在里面，对训练样本多次采样，分别训练出多个不同的模型，然后做综合来减小模型的方差
比如随机森林
Bagging所采用的基分类器最好是本身对样本分布较为敏感的（即不稳定的分类器），这样bagging才有用武之地


>最常用的基分类器是决策树，原因：
1. 更方便地把样本权重整合到训练过程中，不需要使用过采样的方法来调整样本权重
2. 表达能力和泛化能力可以通过调节树的层数来做折中
3. 不同子集样本集合生成的决策树基分类器随机性较大

> 偏差：模型预测值和真实值差的期望    由偏差带来的误差通常在在训练误差上就能体现出来
方差：所有模型预测值的方差    由方差带来的误差通常体现在测试误差相对于训练误差的增量上

### GBDT
梯度提升决策树。Boosting的一种，体现了从错误中学习的理念，基于决策树预测的残差进行迭代的学习
>Gradient Boosting是Boosting的一大类算法，其基本思想是根据当前模型损失函数的负梯度信息来训练新加入的弱分类器，然后将训练好的弱分类器以累加的形式结合到现有模型中
在每一轮迭代中，首先计算出当前模型在所有样本上的负梯度，然后以该值为目标训练一个新的弱分类器进行拟合并计算出该弱分类器的权重，最终实现对模型的更新
GBDT中使用的决策树通常为CART

> 梯度提升 and 梯度下降
梯度下降：模型以参数化的形式表示，模型的更新等价于参数的更新
梯度提升：模型并不需要参数化表示，直接定义在函数空间中，从而大大扩展了可以使用的模型种类

>GBDT优缺点
优点：1. 预测阶段计算速度快，树与树之前可以并行化计算；2. 分布稠密的数据集上，泛化能力和表达能力都很好；3. 采用决策树作为弱分类器使得其具有较好的解释性和鲁棒性，能自动发现特征间的高阶关系，不需要对数据进行特殊预处理
缺点：1. 在高维稀疏数据上表现不如SVM或者神经网络； 2. 处理文本分类特征问题上的优势不如处理数值特征时明显；3. 训练过程需要串行训练

### XGboost
实现了对GBDT算法的改进
原始的GBDT算法基于经验损失函数的负梯度来构造新的决策树，只是在决策树构建完成后再进行剪枝
而XGBoost在决策树构建阶段就加入了正则项
XGBoost有特定的准则来选取最优分裂
>与GBDT的联系和区别：
1. GBDT是机器学习算法；XGBoost是该算法的工程实现
2. 使用CART作为基分类器时，XGBoost显示地加入了正则项来控制模型的复杂度，有利于防止过拟合
3. GBDT在模型训练时只使用了损失函数的一阶导数信息，而XGBoost对损失函数进行二阶泰勒展开，可以同时使用一阶和二阶导数
4. 传统的GBDT采用CART作为基分类器，而XGBoost支持多种类型的基分类器，比如线性的
5. 传统的GBDT在每轮迭代时使用全部数据，而XGBoost则可以支持对数据采用
6. 传统的GBDT没有设计对缺失值进行处理，而XGBoost能够自动学习出缺失值处理侧拉



